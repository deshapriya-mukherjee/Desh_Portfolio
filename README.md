# Desh_Portfolio
Data Scientist

# [Project 1: Co2Emission_Multilinear_Regression_Project: Project Overview](https://github.com/deshapriyamukherjee/Co2Emission_Multilinear_Regression_Project.git)
## Multivariate Regression Model using Scikit-learn Package.
1. Data collection from the source.
2. Data Exploration and Visualization.
3. Data Cleaning, Outlier Detection, Skewness Checking,Correlated independent variables ommiting, Dealing with Contineous and Categorical Independent Variables, Proper Scaling, Making Linear Regression Model and Finally Checking Different Types of Errors (MAE,RMSE,MSE) and Best fitted Model to Check the R-Squared.
4. Engineered features from the text of each job description to quantify the value companies put on python, excel.

# [Project 2: Customer Churn Project with Logistic Regression Machine Learning Model: Project Overview](https://github.com/deshapriya-mukherjee/Customer-Churn-Project.git)
## Built the Model using Logistic Regression from Scikit-learn package. This function implements Logistic Regression and can use different numerical Optimizers to find Parameters.
1. Data collection from the source.
2. Data Exploration and Visualization.
3. Data Cleaning, Outlier Detection, Skewness Checking,Correlated independent variables ommiting, Dealing with Contineous and Categorical Independent Variables, Proper Scaling, Making Logistic Regression Model and Finally Checking accuracy of classifier is to look at confusion matrix.
4. The classifier correctly predicted 24 of them as 0, and one of them wrongly as 1. So, it has done a good job in predicting the customers with churn value 0. A good thing about confusion matrix is that shows the model’s ability to correctly predict or separate the classes.  In specific case of binary classifier, such as this example,  we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives.
5. The F1score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.
6. And finally, we can tell the average accuracy for this classifier is the average of the f1-score for both labels, which is 0.76 in our case.
7. Engineered features from the text of each job description to quantify the value companies put on python, excel.

# [Project 3: Cancer Cell Detection using Support Vector Machine: Project Overview](https://github.com/deshapriya-mukherjee/Cancer-Cell-Detection.git)
## The dataset consists of several hundred Human Cell sample records, each of which contains the values of a set of cell characteristics. I use SVM (Support Vector Machines from Scikit-learn) to Build and Train a Model using human cell records, and classify cells to whether the samples are Benign or Malignant.
1. Data collection from the source.
2. Data Exploration and Visualization.
3. Data Cleaning, Proper Scaling, Making Support Vector Machines from Scikit-learn Model and Finally Checking accuracy of classifier is to look at confusion matrix.
4. The classifier correctly predicted almost 133 and wrongly only 4.So, it has done a good job in predicting the correct cancer cell detection. we can interpret these numbers as the count of true positives, false positives, true negatives, and false negatives.
5. The F1score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.
6. And finally, we can tell the average accuracy for this classifier is the average of the f1-score for both labels, which is 0.97 in our case.
7. Engineered features from the text of each job description to quantify the value companies put on python, excel.


# [Project 4 Employee Absenteeism Project: : Project Overview](https://github.com/deshapriya-mukherjee/Employee-Absenteeism.git)
## Our task is to build regression models which will classify the quality what changes company should bring to reduce the number of absenteeism depending on multiple numerical and categorical historical observations. Given below is a sample of the data set that we are using to predict the absenteeism hour (Total 21 Variables and 740 Observations).
1. Data collection from the source.
2. Data Exploration and Visualization and finding the data insights from the source data.
3. Data Cleaning (Missing value imputations, outlier removing, dealing with categorical variables using level encoder dummy variables), data scaling.
4. Data splitting with proper training,validation and test purpose.
5. Applying four most popular algorithm (Classification model) Logistic Regression, Random Forest, Gradient Boosting, XG Boost alrorithms and find out the best fitted model using training and test score and check how many variables properly predicted or not using confusion matrix and finally visualize the ROC curve how the model perform.
6. Engineered features from the text of each job description to quantify the value companies put on python, excel. 
